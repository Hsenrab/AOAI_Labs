{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Skill Example WIP\n",
    "\n",
    "The scripts rely on API keys for the connections\n",
    "\n",
    "This notebook creates the following objects on your search service:\n",
    "\n",
    "+ data source\n",
    "+ skillset\n",
    "+ search index\n",
    "+ indexer\n",
    "\n",
    "Once you've run all cells the data wil begin being indexed but the query won't return results until the indexer is finished and the search index is loaded. \n",
    "</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- [Azure Storage](https://learn.microsoft.com/azure/storage/common/storage-account-create)\n",
    "   - Create a new container in your storage account. Make it identifiable to you.\n",
    "   - Upload your data set (pdfs)\n",
    "\n",
    "*Common data set to use is the nasa e-book - Upload the [PDFs from this folder](https://github.com/Azure-Samples/azure-search-sample-data/tree/main/nasa-e-book/earth_book_2019_text_pages)*\n",
    "\n",
    "##### *(The rest of the pre-requisites will be completed if you followed 00_Setup)*\n",
    "\n",
    "- [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/how-to/create-resource)\n",
    "(You may have already created them in previous notebooks)\n",
    "  - Deploy an embedding model text-embedding-ada-002\n",
    "  - Update the .env file with OPENAI_API_KEY (In the portal - Resource Management - Keys and Endpoint)\n",
    "  - Update the .env file with OPENAI_API_ENDPOINT \n",
    "  \n",
    "\n",
    "- [Azure AI Services multiservice account](https://learn.microsoft.com/azure/ai-services/multi-service-resource), in the same region as Azure AI Search. This resource is used for the Entity Recognition skill that detects locations in your content.\n",
    "  - Update the .env file with AZURE_AI_KEY (In the portal - Resource Management - Keys and Endpoint)\n",
    "  \n",
    "\n",
    "- [Azure AI Search](https://learn.microsoft.com/azure/search/search-create-service-portal)\n",
    "(You may have already created them in previous notebooks)\n",
    "  - Basic tier or higher is recommended.\n",
    "  - Choose the same region as Azure OpenAI.\n",
    "  - Enable semantic ranking.\n",
    "  - Enable role-based access control.\n",
    "  - Enable a system identity for Azure AI Search.\n",
    "  - Update the .env file with AI_SEARCH_KEY  (In the portal go to resources then Settings, Keys on the left)\n",
    "  - Update the .env file with AI_SEARCH_ENDPOINT  \n",
    "  \n",
    "Make sure you know the name of the deployed models, and have the endpoints for all Azure resources at hand. You will provide this information in the steps that follow.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install azure-search-documents==11.5.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the directory of the currently running file\n",
    "file_dir = \"C:\\\\Users\\\\hannahhowell\\\\OneDrive - Microsoft\\\\Documents\\Git\\\\AOAI_Labs\\\\AzureAISearchLab\"\n",
    "# Get the current working directory\n",
    "current_cwd = os.getcwd()\n",
    "\n",
    "# Check if the current working directory is the same as the file directory\n",
    "if current_cwd != file_dir:\n",
    "    # Change the current working directory to the file directory\n",
    "    os.chdir(file_dir)\n",
    "    print(f\"Changed current working directory to: {file_dir}\")\n",
    "else:\n",
    "    print(\"Already in the correct directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Set container name to name of newly created container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_name = \"pdfscompiancecatalyst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "load_dotenv()\n",
    "\n",
    "# Check the environment variables are set and assign them to variables.\n",
    "AI_SEARCH_ENDPOINT = os.getenv('AI_SEARCH_ENDPOINT')\n",
    "AI_SEARCH_KEY = os.getenv('AI_SEARCH_KEY')\n",
    "\n",
    "BLOB_STORAGE_ACCOUNT_CONNECTION_STRING = os.getenv('BLOB_STORAGE_ACCOUNT_CONNECTION_STRING')\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "OPENAI_API_ENDPOINT = os.getenv('OPENAI_API_ENDPOINT')\n",
    "\n",
    "AZURE_AI_KEY = os.getenv('AZURE_AI_KEY')\n",
    "\n",
    "# Ensure all required environment variables are set\n",
    "if not all([AI_SEARCH_ENDPOINT, AI_SEARCH_KEY, BLOB_STORAGE_ACCOUNT_CONNECTION_STRING, OPENAI_API_KEY, OPENAI_API_ENDPOINT, AZURE_AI_KEY]):\n",
    "    missing_vars = [var for var, val in zip(['AI_SEARCH_ENDPOINT', 'AI_SEARCH_KEY', 'BLOB_STORAGE_ACCOUNT_CONNECTION_STRING', 'OPENAI_API_KEY', 'OPENAI_API_ENDPOINT', 'AZURE_AI_KEY'], \n",
    "                                            [AI_SEARCH_ENDPOINT, AI_SEARCH_KEY, BLOB_STORAGE_ACCOUNT_CONNECTION_STRING, OPENAI_API_KEY, OPENAI_API_ENDPOINT, AZURE_AI_KEY]) if not val]\n",
    "    raise ValueError(f\"Environment variables {', '.join(missing_vars)} must be set.\")\n",
    "\n",
    "# Print the environment variables\n",
    "print(f\"AI_SEARCH_ENDPOINT: {AI_SEARCH_ENDPOINT}\")\n",
    "print(f\"AI_SEARCH_KEY: {AI_SEARCH_KEY}\")\n",
    "print(f\"BLOB_STORAGE_ACCOUNT_CONNECTION_STRING: {BLOB_STORAGE_ACCOUNT_CONNECTION_STRING}\")\n",
    "print(f\"OPENAI_API_KEY: {OPENAI_API_KEY}\")\n",
    "print(f\"OPENAI_API_ENDPOINT: {OPENAI_API_ENDPOINT}\")\n",
    "print(f\"AZURE_AI_KEY: {AZURE_AI_KEY}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    SearchIndex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Data Source (Blob Container containting the pdfs)\n",
    "\n",
    "Although only  PDF files are used here, this can be done at a much larger scale and Azure AI Search supports a range of other file formats including: Microsoft Office (DOCX/DOC, XSLX/XLS, PPTX/PPT, MSG), HTML, XML, ZIP, and plain text files (including JSON).\n",
    "Azure Search support the following sources: [Data Sources Gallery](https://learn.microsoft.com/EN-US/AZURE/search/search-data-sources-gallery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataSourceConnection\n",
    ")\n",
    "\n",
    "# Create a data source \n",
    "indexer_client = SearchIndexerClient(endpoint=AI_SEARCH_ENDPOINT, credential=AzureKeyCredential(AI_SEARCH_KEY))\n",
    "container = SearchIndexerDataContainer(name=container_name)\n",
    "data_source_connection = SearchIndexerDataSourceConnection(\n",
    "    name=container_name+\"-connection\",\n",
    "    type=\"azureblob\",\n",
    "    connection_string=BLOB_STORAGE_ACCOUNT_CONNECTION_STRING,\n",
    "    container=container\n",
    ")\n",
    "data_source = indexer_client.create_or_update_data_source_connection(data_source_connection)\n",
    "\n",
    "print(f\"Data source '{data_source.name}' created or updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    SearchIndex\n",
    ")\n",
    "\n",
    "AZURE_SEARCH_CREDENTIAL = AzureKeyCredential(AI_SEARCH_KEY)\n",
    "\n",
    "# Create a search index  \n",
    "index_name = container_name+\"-index\"\n",
    "index_client = SearchIndexClient(endpoint=AI_SEARCH_ENDPOINT, credential=AZURE_SEARCH_CREDENTIAL)  \n",
    "fields = [\n",
    "    SearchField(name=\"parent_id\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"title\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"chunk_id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True, analyzer_name=\"keyword\"),  \n",
    "    SearchField(name=\"chunk\", type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False),\n",
    "    SearchField(name=\"page_number\", type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False),\n",
    "    SearchField(name=\"text_vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\")\n",
    "    ]  \n",
    "  \n",
    "# Configure the vector search configuration  \n",
    "vector_search = VectorSearch(  \n",
    "    algorithms=[  \n",
    "        HnswAlgorithmConfiguration(name=\"myHnsw\"),\n",
    "    ],  \n",
    "    profiles=[  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"myHnswProfile\",  \n",
    "            algorithm_configuration_name=\"myHnsw\",  \n",
    "            vectorizer_name=\"myOpenAI\",  \n",
    "        )\n",
    "    ],  \n",
    "    vectorizers=[  \n",
    "        AzureOpenAIVectorizer(  \n",
    "            vectorizer_name=\"myOpenAI\",  \n",
    "            kind=\"azureOpenAI\",  \n",
    "            parameters=AzureOpenAIVectorizerParameters(  \n",
    "                resource_url=OPENAI_API_ENDPOINT,  \n",
    "                deployment_name=os.getenv(\"EMBEDDINGS_MODEL_NAME\"),\n",
    "                model_name=\"text-embedding-ada-002\",\n",
    "                api_key=OPENAI_API_KEY\n",
    "            ),\n",
    "        ),  \n",
    "    ], \n",
    ")  \n",
    "  \n",
    "# Create the search index\n",
    "index = SearchIndex(name=index_name, fields=fields, vector_search=vector_search)  \n",
    "result = index_client.create_or_update_index(index)  \n",
    "print(f\"{result.name} created\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Skill Set\n",
    "One option for data pre-processing is to do it as part of the indexing by using a skillset.\n",
    "A Skillset is a set of steps in which AI Services can be used to enrich the documents by extracting information, applying OCR, translating, etc.\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/search/cognitive-search-working-with-skillsets\n",
    "https://learn.microsoft.com/en-us/azure/search/cognitive-search-predefined-skills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import ( \n",
    "    SplitSkill,\n",
    "    InputFieldMappingEntry,\n",
    "    OutputFieldMappingEntry,\n",
    "    AzureOpenAIEmbeddingSkill,\n",
    "    EntityRecognitionSkill,\n",
    "    SearchIndexerIndexProjection,\n",
    "    SearchIndexerIndexProjectionSelector,\n",
    "    SearchIndexerIndexProjectionsParameters,\n",
    "    IndexProjectionMode,\n",
    "    SearchIndexerSkillset,\n",
    "    CognitiveServicesAccountKey,\n",
    "    WebApiSkill,\n",
    "    SearchIndexerDataUserAssignedIdentity\n",
    ")\n",
    "\n",
    "# Create a skillset  \n",
    "skillset_name = container_name+\"-skillset\"\n",
    "\n",
    "split_skill = SplitSkill(  \n",
    "    description=\"Split skill to chunk documents\",  \n",
    "    text_split_mode=\"pages\",  \n",
    "    context=\"/document\",  \n",
    "    maximum_page_length=2000,  \n",
    "    page_overlap_length=500,  \n",
    "    inputs=[  \n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/content\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        OutputFieldMappingEntry(name=\"textItems\", target_name=\"pages\")  \n",
    "    ],  \n",
    ")\n",
    "\n",
    "\n",
    "webapi_skill = WebApiSkill(\n",
    "    description=\"GPT4o skill to generate text from images\",\n",
    "    uri=\"https://genai-extact-from-pdf.azurewebsites.net/api/extracttomarkdown\",\n",
    "    context=\"/document\",\n",
    "    inputs=[\n",
    "        InputFieldMappingEntry(name=\"docUrl\", source=\"/document/metadata_storage_path\"),\n",
    "        InputFieldMappingEntry(name=\"docSAS\", source=\"/document/metadata_storage_sas_token\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        OutputFieldMappingEntry(name=\"markdown_chunks\", target_name=\"chunks\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "embedding_skill = AzureOpenAIEmbeddingSkill(  \n",
    "    description=\"Skill to generate embeddings via Azure OpenAI\",  \n",
    "    context=\"/document/markdown_chunks/*\",\n",
    "    resource_url=OPENAI_API_ENDPOINT,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    deployment_name=os.getenv(\"EMBEDDINGS_MODEL_NAME\"),\n",
    "    model_name=\"text-embedding-ada-002\",\n",
    "    dimensions=1536,\n",
    "    inputs=[  \n",
    "        InputFieldMappingEntry(name=\"text\", source=\"/document/markdown_chunks/*/markdown\"),  \n",
    "    ],  \n",
    "    outputs=[  \n",
    "        OutputFieldMappingEntry(name=\"embedding\", target_name=\"text_vector\")  \n",
    "    ],  \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "skills = [split_skill, webapi_skill, embedding_skill]\n",
    "\n",
    "\n",
    "index_projections = SearchIndexerIndexProjection(  \n",
    "    selectors=[  \n",
    "        SearchIndexerIndexProjectionSelector(  \n",
    "            target_index_name=index_name,  \n",
    "            parent_key_field_name=\"parent_id\",  \n",
    "            source_context=\"/document/markdown_chunks/*\",  \n",
    "            mappings=[  \n",
    "                InputFieldMappingEntry(name=\"chunk\", source=\"/document/markdown_chunks/*/markdown\"),  \n",
    "                InputFieldMappingEntry(name=\"text_vector\", source=\"/document/markdown_chunks/*/text_vector\"),\n",
    "                InputFieldMappingEntry(name=\"page_number\", source=\"/document/markdown_chunks/*/page_number\"),\n",
    "                InputFieldMappingEntry(name=\"title\", source=\"/document/metadata_storage_name\"),  \n",
    "            ],  \n",
    "        ),  \n",
    "    ],  \n",
    "    parameters=SearchIndexerIndexProjectionsParameters(  \n",
    "        projection_mode=IndexProjectionMode.SKIP_INDEXING_PARENT_DOCUMENTS  \n",
    "    ),  \n",
    ") \n",
    "\n",
    "cognitive_services_account = CognitiveServicesAccountKey(key=AZURE_AI_KEY)\n",
    "\n",
    "\n",
    "skillset = SearchIndexerSkillset(  \n",
    "    name=skillset_name,  \n",
    "    description=\"Skillset to chunk documents and generating embeddings\",  \n",
    "    skills=skills,\n",
    "    index_projection=index_projections,\n",
    "    cognitive_services_account=cognitive_services_account\n",
    ")\n",
    "\n",
    "client = SearchIndexerClient(endpoint=AI_SEARCH_ENDPOINT, credential=AZURE_SEARCH_CREDENTIAL)  \n",
    "client.create_or_update_skillset(skillset)  \n",
    "print(f\"{skillset.name} created\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexer,\n",
    "    FieldMapping\n",
    ")\n",
    "\n",
    "# Create an indexer \n",
    "indexer_name = container_name+\"-indexer\" \n",
    "\n",
    "indexer_parameters = None\n",
    "\n",
    "indexer = SearchIndexer(  \n",
    "    name=indexer_name,  \n",
    "    description=\"Indexer to index documents and generate embeddings\",  \n",
    "    skillset_name=skillset_name,  \n",
    "    target_index_name=index_name,  \n",
    "    data_source_name=data_source.name,\n",
    "    # Map the metadata_storage_name field to the title field in the index to display the PDF title in the search results  \n",
    "    field_mappings=[FieldMapping(source_field_name=\"metadata_storage_name\", target_field_name=\"title\")],\n",
    "    parameters=indexer_parameters\n",
    ")  \n",
    "\n",
    "# Create and run the indexer  \n",
    "indexer_client = SearchIndexerClient(endpoint=AI_SEARCH_ENDPOINT, credential=AZURE_SEARCH_CREDENTIAL) \n",
    "\n",
    "indexer_result = indexer_client.create_or_update_indexer(indexer)\n",
    "\n",
    "\n",
    "print(f' {indexer_name} is created and running. Give the indexer a few minutes before running a query.')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "\n",
    "# Vector Search using text-to-vector conversion of the querystring\n",
    "query = \"how much of earth is covered by water\"  \n",
    "\n",
    "search_client = SearchClient(endpoint=AI_SEARCH_ENDPOINT, credential=AZURE_SEARCH_CREDENTIAL, index_name=index_name)\n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=1, fields=\"text_vector\", exhaustive=True)\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"parent_id\", \"chunk_id\", \"title\", \"chunk\", \"locations\", \"topWords\"],\n",
    "    top=1\n",
    ")  \n",
    "  \n",
    "for result in results: \n",
    "    print(result)\n",
    "    print(f\"Score: {result['@search.score']}\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Locations: {result['locations']}\")\n",
    "    print(f\"TopWords: {result['topWords']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Set up clients and specify the chat model\n",
    "openai_client = AzureOpenAI(\n",
    "    api_version=\"2024-06-01\",\n",
    "    azure_endpoint=OPENAI_API_ENDPOINT,\n",
    "    api_key=OPENAI_API_KEY\n",
    " )\n",
    "\n",
    "model = os.getenv(\"GPT4_MODEL_NAME\")\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=AI_SEARCH_ENDPOINT,\n",
    "    index_name=index_name,\n",
    "    credential=AzureKeyCredential(AI_SEARCH_KEY)\n",
    " )\n",
    "\n",
    "# Provide instructions to the model\n",
    "GROUNDED_PROMPT=\"\"\"\n",
    "You are an AI assistant that helps users learn from the information found in the source material.\n",
    "Answer the query using only the sources provided below.\n",
    "Use bullets if the answer has multiple points.\n",
    "If the answer is longer than 3 sentences, provide a summary.\n",
    "Answer ONLY with the facts listed in the list of sources below.\n",
    "If there isn't enough information below, say you don't know.\n",
    "Do not generate answers that don't use the sources below.\n",
    "Query: {query}\n",
    "Sources:\\n{sources}\n",
    "\"\"\"\n",
    "\n",
    "# Provide the query. Notice it's sent to both the search engine and the LLM.\n",
    "query=\"What are the Barren Grounds\"\n",
    "\n",
    "# Set up the search results and the chat thread.\n",
    "# Retrieve the selected fields from the search index related to the question.\n",
    "search_results = search_client.search(\n",
    "    search_text=query,\n",
    "    top=1,\n",
    "    select=\"title, chunk, locations\"\n",
    ")\n",
    "sources_formatted = \"\\n\".join([f'{document[\"title\"]}:{document[\"chunk\"]}:{document[\"locations\"]}' for document in search_results])\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": GROUNDED_PROMPT.format(query=query, sources=sources_formatted)\n",
    "        }\n",
    "    ],\n",
    "    model=model\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Optional Experimentation\n",
    "\n",
    "Notice that whatever question you ask the search will bring back info it thinks is relevant.  \n",
    "However if the answer to the question is not in the retrieved data the LLM will respond \"I don't know\".  \n",
    "\n",
    "*For the Nasa Data Set a good question is \"How much of the earths surface is covered with water\"*\n",
    "\n",
    "Currently the search is passing the whole query from the user and using it to do a basic query.\n",
    "- What are the limitations of this method?\n",
    "- What improvements could be made?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promtpflow_examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
