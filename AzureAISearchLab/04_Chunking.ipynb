{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Chunking\n",
    "Partitioning large documents into smaller chunks can help you stay under the maximum token input limits of embedding models. For example, the maximum length of input text for the [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/embeddings) embedding models is 8,191 tokens. Given that each token is around four characters of text for common OpenAI models, this maximum limit is equivalent to around 6,000 words of text. If you're using these models to generate embeddings, it's critical that the input text stays under the limit. Partitioning your content into chunks ensures that your data can be processed by the embedding models used to populate vector stores and text-to-vector query conversions.\n",
    "\n",
    "This notebook walks through the process used in the [chat-with-your-data-solution-accelerator](https://github.com/Azure-Samples/chat-with-your-data-solution-accelerator/tree/main) repo of analysing file content in [AI Document Intelligence](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/overview?view=doc-intel-4.0.0), then breaking it into smaller overlapping chunks using LangChain. These chunks can then be vectorised and stored in a service such as [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) which provides secure information retrieval at scale over user-owned content in traditional and generative AI search applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: Before proceeding, you need to complete the steps in the first `00_Setup` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "First we install the dependencies required in ths notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "\n",
    "%pip install python-dotenv\n",
    "%pip install langchain\n",
    "%pip install azure.ai.formrecognizer\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load credentials\n",
    "Next we load the environment variables needed by the following cells from the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# fixing the env file\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set execution parameters\n",
    "Set the name of the file you uploaded to the blob container (that we are going to chunk) and the container name. The filename is joined with the blob endpoint, container name, and SAS token to create a url to the file. You can also adjust the `chunk_size` and `chunk_overlap` to see their effect on the output later, or leave the defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The name of the file you uploaded to your Blob Storage Datasource\n",
    "# file_name=\"employee_handbook.pdf\"\n",
    "#file_name=\"PHE_Covid-19_consent_form_adults_able_to_consent_v2.pdf\"\n",
    "file_name=\"WHO-2019-nCoV-SRH-Rights-2020.1-eng.pdf\"\n",
    "blob_sas=os.getenv(\"BLOB_SAS_TOKEN\")\n",
    "file_url=os.getenv(\"BLOB_CONTAINER_ENDPOINT\") + \"/\" + file_name + \"?\" + blob_sas\n",
    "\n",
    "print(f\"file url={file_url}\")\n",
    "\n",
    "# You can play with these parameters to see how they affect the output\n",
    "my_chunk_size = 500\n",
    "my_chunk_overlap = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the file in with Document Intelligence\n",
    "[Azure AI Document Intelligence](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/overview?view=doc-intel-4.0.0) is a cloud-based Azure AI service that enables you to build intelligent document processing solutions. Massive amounts of data, spanning a wide variety of data types, are stored in forms and documents. Document Intelligence enables you to effectively manage the velocity at which data is collected and processed and is key to improved operations, informed data-driven decisions, and enlightened innovation.\n",
    "\n",
    "This cell creates a `DocumentAnalysisClient` object using the settings in the credentials file. It then defines some helper functions to do things like converting tables to HTML.\n",
    "\n",
    "The main part of the code calls the *DocumentAnalysisClient* `begin_analyze_document_from_url` method to analyse the file using the pre-defined `layout` [model](https://learn.microsoft.com/en-gb/azure/ai-services/document-intelligence/concept-layout?view=doc-intel-4.0.0&tabs=sample-code). The output of the DocumentAnalysisClient call is then parsed and converted into HTML formatted *pages*, while keeping track of paragraph and table roles and positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\\backend\\batch\\utilities\\helpers\\azure_form_recognizer_helper.py\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "import html\n",
    "import traceback\n",
    "\n",
    "\n",
    "print(f\"form recognizer={os.getenv('FORM_RECOGNIZER_ENDPOINT')}\")\n",
    "\n",
    "document_analysis_client = DocumentAnalysisClient(\n",
    "    endpoint=os.getenv(\"FORM_RECOGNIZER_ENDPOINT\"), \n",
    "    credential=AzureKeyCredential(os.getenv(\"FORM_RECOGNIZER_KEY\"))\n",
    ")\n",
    "\n",
    "form_recognizer_role_to_html = {\n",
    "    \"title\": \"h1\",\n",
    "    \"sectionHeading\": \"h2\",\n",
    "    \"pageHeader\": None,\n",
    "    \"pageFooter\": None,\n",
    "    \"paragraph\": \"p\",\n",
    "}\n",
    "\n",
    "def _table_to_html(table):\n",
    "    table_html = \"<table>\"\n",
    "    rows = [\n",
    "        sorted(\n",
    "            [cell for cell in table.cells if cell.row_index == i],\n",
    "            key=lambda cell: cell.column_index,\n",
    "        )\n",
    "        for i in range(table.row_count)\n",
    "    ]\n",
    "    for row_cells in rows:\n",
    "        table_html += \"<tr>\"\n",
    "        for cell in row_cells:\n",
    "            tag = (\n",
    "                \"th\"\n",
    "                if (cell.kind == \"columnHeader\" or cell.kind == \"rowHeader\")\n",
    "                else \"td\"\n",
    "            )\n",
    "            cell_spans = \"\"\n",
    "            if cell.column_span > 1:\n",
    "                cell_spans += f\" colSpan={cell.column_span}\"\n",
    "            if cell.row_span > 1:\n",
    "                cell_spans += f\" rowSpan={cell.row_span}\"\n",
    "            table_html += f\"<{tag}{cell_spans}>{html.escape(cell.content)}</{tag}>\"\n",
    "        table_html += \"</tr>\"\n",
    "    table_html += \"</table>\"\n",
    "    return table_html\n",
    "\n",
    "\n",
    "offset = 0\n",
    "page_map = []\n",
    "model_id = \"prebuilt-layout\"\n",
    "\n",
    "try:\n",
    "    poller = document_analysis_client.begin_analyze_document_from_url(\n",
    "        model_id, document_url=file_url\n",
    "    )\n",
    "    form_recognizer_results = poller.result()\n",
    "\n",
    "    roles_start = {}\n",
    "    roles_end = {}\n",
    "    for paragraph in form_recognizer_results.paragraphs:\n",
    "        # if paragraph.role!=None:\n",
    "        para_start = paragraph.spans[0].offset\n",
    "        para_end = paragraph.spans[0].offset + paragraph.spans[0].length\n",
    "        roles_start[para_start] = (\n",
    "            paragraph.role if paragraph.role is not None else \"paragraph\"\n",
    "        )\n",
    "        roles_end[para_end] = (\n",
    "            paragraph.role if paragraph.role is not None else \"paragraph\"\n",
    "        )\n",
    "\n",
    "    for page_num, page in enumerate(form_recognizer_results.pages):\n",
    "        tables_on_page = [\n",
    "            table\n",
    "            for table in form_recognizer_results.tables\n",
    "            if table.bounding_regions[0].page_number == page_num + 1\n",
    "        ]\n",
    "\n",
    "        # (if using layout) mark all positions of the table spans in the page\n",
    "        page_offset = page.spans[0].offset\n",
    "        page_length = page.spans[0].length\n",
    "        table_chars = [-1] * page_length\n",
    "        for table_id, table in enumerate(tables_on_page):\n",
    "            for span in table.spans:\n",
    "                # replace all table spans with \"table_id\" in table_chars array\n",
    "                for i in range(span.length):\n",
    "                    idx = span.offset - page_offset + i\n",
    "                    if idx >= 0 and idx < page_length:\n",
    "                        table_chars[idx] = table_id\n",
    "\n",
    "        # build page text by replacing charcters in table spans with table html and replace the characters corresponding to headers with html headers, if using layout\n",
    "        page_text = \"\"\n",
    "        added_tables = set()\n",
    "        for idx, table_id in enumerate(table_chars):\n",
    "            if table_id == -1:\n",
    "                position = page_offset + idx\n",
    "                if position in roles_start.keys():\n",
    "                    role = roles_start[position]\n",
    "                    html_role = form_recognizer_role_to_html.get(role)\n",
    "                    if html_role is not None:\n",
    "                        page_text += f\"<{html_role}>\"\n",
    "                if position in roles_end.keys():\n",
    "                    role = roles_end[position]\n",
    "                    html_role = form_recognizer_role_to_html.get(role)\n",
    "                    if html_role is not None:\n",
    "                        page_text += f\"</{html_role}>\"\n",
    "\n",
    "                page_text += form_recognizer_results.content[page_offset + idx]\n",
    "\n",
    "            elif table_id not in added_tables:\n",
    "                page_text += _table_to_html(tables_on_page[table_id])\n",
    "                added_tables.add(table_id)\n",
    "\n",
    "        page_text += \" \"\n",
    "        page_map.append(\n",
    "            {\"page_number\": page_num, \"offset\": offset, \"page_text\": page_text}\n",
    "        )\n",
    "        offset += len(page_text)\n",
    "\n",
    "    print(f\"page map={page_map}\")\n",
    "\n",
    "except Exception as e:\n",
    "    raise ValueError(f\"Error: {traceback.format_exc()}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse into a document object\n",
    "The following cell takes the *page_map* created in the previous cell and builds a list of `SourceDocument` objects, one for each page in the page_map, which are used to simplify the next phase of processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\\backend\\batch\\utilities\\document_loading\\layout.py\n",
    "\n",
    "from SourceDocument import SourceDocument\n",
    "\n",
    "pages_content = page_map\n",
    "\n",
    "documents = [\n",
    "    SourceDocument(\n",
    "        content=page[\"page_text\"],\n",
    "        source=file_url,\n",
    "        offset=page[\"offset\"],\n",
    "        page_number=page[\"page_number\"],\n",
    "    )\n",
    "    for page in pages_content\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk the document\n",
    "The following cell splits the contents of the entire document into chunks based on the parameters set in the earlier cell using [LangChain text splitters](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/). A final list `chunked_documents` contains `SourceDocument` objects with the specified chunk_size and chunk_overlap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code\\backend\\batch\\utilities\\document_chunking\\layout.py\n",
    "\n",
    "from typing import List\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "# Combine all pages into a single document\n",
    "full_document_content = \"\".join(\n",
    "    list(map(lambda document: document.content, documents))\n",
    ")\n",
    "\n",
    "# Split the document into chunks\n",
    "document_url = documents[0].source\n",
    "splitter = MarkdownTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=my_chunk_size, chunk_overlap=my_chunk_overlap\n",
    ")\n",
    "chunked_content_list = splitter.split_text(full_document_content)\n",
    "\n",
    "# Create a list of SourceDocuments from the chunked content\n",
    "chunked_documents = []\n",
    "chunk_offset = 0\n",
    "for idx, chunked_content in enumerate(chunked_content_list):\n",
    "    chunked_documents.append(\n",
    "        SourceDocument.from_metadata(\n",
    "            content=chunked_content,\n",
    "            document_url=document_url,\n",
    "            metadata={\"offset\": chunk_offset},\n",
    "            idx=idx,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Print each chunk\n",
    "    print(f\"offset={chunk_offset}\")\n",
    "    print(f\"chunk={idx}: {chunked_content}\\n\")\n",
    "\n",
    "    chunk_offset += len(chunked_content)\n",
    "\n",
    "#\"chunked_documents\" variable contains the list of documents split into chunks"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
