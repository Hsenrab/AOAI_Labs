{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install azure-search-documents==11.5.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the directory of the currently running file\n",
    "file_dir = \"C:\\\\Users\\\\hannahhowell\\\\OneDrive - Microsoft\\\\Documents\\Git\\\\AOAI_Labs\\\\AzureAISearchLab\"\n",
    "# Get the current working directory\n",
    "current_cwd = os.getcwd()\n",
    "\n",
    "# Check if the current working directory is the same as the file directory\n",
    "if current_cwd != file_dir:\n",
    "    # Change the current working directory to the file directory\n",
    "    os.chdir(file_dir)\n",
    "    print(f\"Changed current working directory to: {file_dir}\")\n",
    "else:\n",
    "    print(\"Already in the correct directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Set container name to name of newly created container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_name = \"pdfscompiancecatalyst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "load_dotenv()\n",
    "\n",
    "# Check the environment variables are set and assign them to variables.\n",
    "AI_SEARCH_ENDPOINT = os.getenv('AI_SEARCH_ENDPOINT')\n",
    "AI_SEARCH_KEY = os.getenv('AI_SEARCH_KEY')\n",
    "\n",
    "BLOB_STORAGE_ACCOUNT_CONNECTION_STRING = os.getenv('BLOB_STORAGE_ACCOUNT_CONNECTION_STRING')\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "OPENAI_API_ENDPOINT = os.getenv('OPENAI_API_ENDPOINT')\n",
    "\n",
    "AZURE_AI_KEY = os.getenv('AZURE_AI_KEY')\n",
    "\n",
    "# Ensure all required environment variables are set\n",
    "if not all([AI_SEARCH_ENDPOINT, AI_SEARCH_KEY, BLOB_STORAGE_ACCOUNT_CONNECTION_STRING, OPENAI_API_KEY, OPENAI_API_ENDPOINT, AZURE_AI_KEY]):\n",
    "    missing_vars = [var for var, val in zip(['AI_SEARCH_ENDPOINT', 'AI_SEARCH_KEY', 'BLOB_STORAGE_ACCOUNT_CONNECTION_STRING', 'OPENAI_API_KEY', 'OPENAI_API_ENDPOINT', 'AZURE_AI_KEY'], \n",
    "                                            [AI_SEARCH_ENDPOINT, AI_SEARCH_KEY, BLOB_STORAGE_ACCOUNT_CONNECTION_STRING, OPENAI_API_KEY, OPENAI_API_ENDPOINT, AZURE_AI_KEY]) if not val]\n",
    "    raise ValueError(f\"Environment variables {', '.join(missing_vars)} must be set.\")\n",
    "\n",
    "# Print the environment variables\n",
    "print(f\"AI_SEARCH_ENDPOINT: {AI_SEARCH_ENDPOINT}\")\n",
    "print(f\"AI_SEARCH_KEY: {AI_SEARCH_KEY}\")\n",
    "print(f\"BLOB_STORAGE_ACCOUNT_CONNECTION_STRING: {BLOB_STORAGE_ACCOUNT_CONNECTION_STRING}\")\n",
    "print(f\"OPENAI_API_KEY: {OPENAI_API_KEY}\")\n",
    "print(f\"OPENAI_API_ENDPOINT: {OPENAI_API_ENDPOINT}\")\n",
    "print(f\"AZURE_AI_KEY: {AZURE_AI_KEY}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    SearchIndex\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Data Source (Blob Container containting the pdfs)\n",
    "\n",
    "Although only  PDF files are used here, this can be done at a much larger scale and Azure AI Search supports a range of other file formats including: Microsoft Office (DOCX/DOC, XSLX/XLS, PPTX/PPT, MSG), HTML, XML, ZIP, and plain text files (including JSON).\n",
    "Azure Search support the following sources: [Data Sources Gallery](https://learn.microsoft.com/EN-US/AZURE/search/search-data-sources-gallery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexerDataContainer,\n",
    "    SearchIndexerDataSourceConnection\n",
    ")\n",
    "\n",
    "# Create a data source \n",
    "indexer_client = SearchIndexerClient(endpoint=AI_SEARCH_ENDPOINT, credential=AzureKeyCredential(AI_SEARCH_KEY))\n",
    "container = SearchIndexerDataContainer(name=container_name)\n",
    "data_source_connection = SearchIndexerDataSourceConnection(\n",
    "    name=container_name+\"-connection\",\n",
    "    type=\"azureblob\",\n",
    "    connection_string=BLOB_STORAGE_ACCOUNT_CONNECTION_STRING,\n",
    "    container=container\n",
    ")\n",
    "data_source = indexer_client.create_or_update_data_source_connection(data_source_connection)\n",
    "\n",
    "print(f\"Data source '{data_source.name}' created or updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_name = \"mapping-test\"  # changing so I do not overwrite the existing indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    AzureOpenAIVectorizer,\n",
    "    AzureOpenAIVectorizerParameters,\n",
    "    SearchIndex\n",
    ")\n",
    "\n",
    "AZURE_SEARCH_CREDENTIAL = AzureKeyCredential(AI_SEARCH_KEY)\n",
    "\n",
    "# Create a search index  \n",
    "index_name = container_name+\"-index\"\n",
    "index_client = SearchIndexClient(endpoint=AI_SEARCH_ENDPOINT, credential=AZURE_SEARCH_CREDENTIAL)  \n",
    "fields = [\n",
    "    SearchField(name=\"parent_id\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"title\", type=SearchFieldDataType.String),\n",
    "    SearchField(name=\"subfield_id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True, analyzer_name=\"keyword\"),  \n",
    "    SearchField(name=\"subfield\", type=SearchFieldDataType.String, sortable=False, filterable=False, facetable=False),\n",
    "    ]  \n",
    " \n",
    "  \n",
    "# Create the search index\n",
    "index = SearchIndex(name=index_name, fields=fields)  \n",
    "result = index_client.create_or_update_index(index)  \n",
    "print(f\"{result.name} created\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Skill Set\n",
    "One option for data pre-processing is to do it as part of the indexing by using a skillset.\n",
    "A Skillset is a set of steps in which AI Services can be used to enrich the documents by extracting information, applying OCR, translating, etc.\n",
    "\n",
    "https://learn.microsoft.com/en-us/azure/search/cognitive-search-working-with-skillsets\n",
    "https://learn.microsoft.com/en-us/azure/search/cognitive-search-predefined-skills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import ( \n",
    "    SplitSkill,\n",
    "    InputFieldMappingEntry,\n",
    "    OutputFieldMappingEntry,\n",
    "    AzureOpenAIEmbeddingSkill,\n",
    "    EntityRecognitionSkill,\n",
    "    SearchIndexerIndexProjection,\n",
    "    SearchIndexerIndexProjectionSelector,\n",
    "    SearchIndexerIndexProjectionsParameters,\n",
    "    IndexProjectionMode,\n",
    "    SearchIndexerSkillset,\n",
    "    CognitiveServicesAccountKey,\n",
    "    WebApiSkill,\n",
    "    SearchIndexerDataUserAssignedIdentity\n",
    ")\n",
    "\n",
    "# Create a skillset  \n",
    "skillset_name = container_name+\"-skillset\"\n",
    "\n",
    "onetoone_skill = WebApiSkill(\n",
    "    description=\"Skill to generate images from pdf\",\n",
    "    uri=\"https://mappingexample.azurewebsites.net/api/onetoone\",\n",
    "    context=\"/document\",\n",
    "    inputs=[\n",
    "        InputFieldMappingEntry(name=\"input\", source=\"/document/metadata_storage_path\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        OutputFieldMappingEntry(name=\"new_field\", target_name=\"new_field\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "divide_skill = WebApiSkill(\n",
    "    description=\"Skill to generate images from pdf\",\n",
    "    uri=\"https://mappingexample.azurewebsites.net/api/divide\",\n",
    "    context=\"/document\",\n",
    "    inputs=[\n",
    "        InputFieldMappingEntry(name=\"input\", source=\"/document/new_field\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        OutputFieldMappingEntry(name=\"new_list\", target_name=\"new_list\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "onetoone_skill_again = WebApiSkill(\n",
    "    description=\"Skill to generate images from pdf\",\n",
    "    uri=\"https://mappingexample.azurewebsites.net/api/onetoone\",\n",
    "    context=\"/document/new_list/*\",\n",
    "    inputs=[\n",
    "        InputFieldMappingEntry(name=\"input\", source=\"/document/new_list/*/item\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        OutputFieldMappingEntry(name=\"new_field\", target_name=\"new_field\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "skills = [onetoone_skill, divide_skill, onetoone_skill_again]\n",
    "\n",
    "\n",
    "index_projections = SearchIndexerIndexProjection(  \n",
    "    selectors=[  \n",
    "        SearchIndexerIndexProjectionSelector(  \n",
    "            target_index_name=index_name,  \n",
    "            parent_key_field_name=\"parent_id\",  \n",
    "            source_context=\"/document/new_list/*\",  \n",
    "            mappings=[  \n",
    "                InputFieldMappingEntry(name=\"subfield\", source=\"/document/changed_list/*\"), \n",
    "            ],  \n",
    "        ),  \n",
    "    ],  \n",
    "    parameters=SearchIndexerIndexProjectionsParameters(  \n",
    "        projection_mode=IndexProjectionMode.SKIP_INDEXING_PARENT_DOCUMENTS  \n",
    "    ),  \n",
    ") \n",
    "\n",
    "cognitive_services_account = CognitiveServicesAccountKey(key=AZURE_AI_KEY)\n",
    "\n",
    "\n",
    "skillset = SearchIndexerSkillset(  \n",
    "    name=skillset_name,  \n",
    "    description=\"Skillset to chunk documents and generating embeddings\",  \n",
    "    skills=skills,\n",
    "    index_projection=index_projections,\n",
    "    cognitive_services_account=cognitive_services_account\n",
    ")\n",
    "\n",
    "client = SearchIndexerClient(endpoint=AI_SEARCH_ENDPOINT, credential=AZURE_SEARCH_CREDENTIAL)  \n",
    "client.create_or_update_skillset(skillset)  \n",
    "print(f\"{skillset.name} created\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndexer,\n",
    "    FieldMapping\n",
    ")\n",
    "\n",
    "# Create an indexer \n",
    "indexer_name = container_name+\"-indexer\" \n",
    "\n",
    "indexer_parameters = None\n",
    "\n",
    "indexer = SearchIndexer(  \n",
    "    name=indexer_name,  \n",
    "    description=\"Indexer to index documents and generate embeddings\",  \n",
    "    skillset_name=skillset_name,  \n",
    "    target_index_name=index_name,  \n",
    "    data_source_name=data_source.name,\n",
    "    # Map the metadata_storage_name field to the title field in the index to display the PDF title in the search results  \n",
    "    field_mappings=[FieldMapping(source_field_name=\"metadata_storage_name\", target_field_name=\"title\")],\n",
    "    parameters=indexer_parameters\n",
    ")  \n",
    "\n",
    "# Create and run the indexer  \n",
    "indexer_client = SearchIndexerClient(endpoint=AI_SEARCH_ENDPOINT, credential=AZURE_SEARCH_CREDENTIAL) \n",
    "\n",
    "indexer_result = indexer_client.create_or_update_indexer(indexer)\n",
    "\n",
    "\n",
    "print(f' {indexer_name} is created and running. Give the indexer a few minutes before running a query.')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "\n",
    "# Vector Search using text-to-vector conversion of the querystring\n",
    "query = \"how much of earth is covered by water\"  \n",
    "\n",
    "search_client = SearchClient(endpoint=AI_SEARCH_ENDPOINT, credential=AZURE_SEARCH_CREDENTIAL, index_name=index_name)\n",
    "vector_query = VectorizableTextQuery(text=query, k_nearest_neighbors=1, fields=\"text_vector\", exhaustive=True)\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    vector_queries= [vector_query],\n",
    "    select=[\"parent_id\", \"chunk_id\", \"title\", \"chunk\", \"locations\", \"topWords\"],\n",
    "    top=1\n",
    ")  \n",
    "  \n",
    "for result in results: \n",
    "    print(result)\n",
    "    print(f\"Score: {result['@search.score']}\")\n",
    "    print(f\"Title: {result['title']}\")\n",
    "    print(f\"Locations: {result['locations']}\")\n",
    "    print(f\"TopWords: {result['topWords']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Set up clients and specify the chat model\n",
    "openai_client = AzureOpenAI(\n",
    "    api_version=\"2024-06-01\",\n",
    "    azure_endpoint=OPENAI_API_ENDPOINT,\n",
    "    api_key=OPENAI_API_KEY\n",
    " )\n",
    "\n",
    "model = os.getenv(\"GPT4_MODEL_NAME\")\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=AI_SEARCH_ENDPOINT,\n",
    "    index_name=index_name,\n",
    "    credential=AzureKeyCredential(AI_SEARCH_KEY)\n",
    " )\n",
    "\n",
    "# Provide instructions to the model\n",
    "GROUNDED_PROMPT=\"\"\"\n",
    "You are an AI assistant that helps users learn from the information found in the source material.\n",
    "Answer the query using only the sources provided below.\n",
    "Use bullets if the answer has multiple points.\n",
    "If the answer is longer than 3 sentences, provide a summary.\n",
    "Answer ONLY with the facts listed in the list of sources below.\n",
    "If there isn't enough information below, say you don't know.\n",
    "Do not generate answers that don't use the sources below.\n",
    "Query: {query}\n",
    "Sources:\\n{sources}\n",
    "\"\"\"\n",
    "\n",
    "# Provide the query. Notice it's sent to both the search engine and the LLM.\n",
    "query=\"What are the Barren Grounds\"\n",
    "\n",
    "# Set up the search results and the chat thread.\n",
    "# Retrieve the selected fields from the search index related to the question.\n",
    "search_results = search_client.search(\n",
    "    search_text=query,\n",
    "    top=1,\n",
    "    select=\"title, chunk, locations\"\n",
    ")\n",
    "sources_formatted = \"\\n\".join([f'{document[\"title\"]}:{document[\"chunk\"]}:{document[\"locations\"]}' for document in search_results])\n",
    "\n",
    "response = openai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": GROUNDED_PROMPT.format(query=query, sources=sources_formatted)\n",
    "        }\n",
    "    ],\n",
    "    model=model\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Optional Experimentation\n",
    "\n",
    "Notice that whatever question you ask the search will bring back info it thinks is relevant.  \n",
    "However if the answer to the question is not in the retrieved data the LLM will respond \"I don't know\".  \n",
    "\n",
    "*For the Nasa Data Set a good question is \"How much of the earths surface is covered with water\"*\n",
    "\n",
    "Currently the search is passing the whole query from the user and using it to do a basic query.\n",
    "- What are the limitations of this method?\n",
    "- What improvements could be made?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
