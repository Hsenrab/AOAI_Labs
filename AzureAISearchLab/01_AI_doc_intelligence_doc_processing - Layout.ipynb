{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Azure AI Document Intelligence - Layout Model\n",
    "> https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/overview?view=doc-intel-4.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Create an AI Document Intelligence resource and set up environment to run notebook\n",
    "\n",
    "### Prerequisite\n",
    "\n",
    "#### Create Document Intelligence resource\n",
    "To create a AI Document Intelligence resource in your Azure subscription:\n",
    "Please follow the steps as specified https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/create-document-intelligence-resource?view=doc-intel-4.0.0\n",
    "\n",
    "#### Create a storage account.\n",
    "1. https://learn.microsoft.com/en-us/azure/storage/blobs/blob-containers-portal\n",
    "    - Primary Service - Azure Blob Storage \n",
    "    - Primary Workload - Other \n",
    "    - Standard performance \n",
    "    - As this is a demo we can also choose Locally Redundant Storage \n",
    "\n",
    "1. Create a container called *data* in the `Container` blade.\n",
    "1. Upload a file to analyse into your container by clicking on the newly created container and selecting the `Upload` button in the ribbon.\n",
    "\n",
    "\n",
    "Navigate to the file you want to analyse in the portal - it should be uploaded into blob storage  \n",
    "On the far right there are 3 dots from there you can generate a SAS url for the file.\n",
    "in your .env file populate BLOB_SAS_URL with the SAS url.\n",
    "\n",
    "#### Environment\n",
    "\n",
    "**AML workspace**: Please ensure you have Python 3.10 version or above ie select **Python 3.10 - SDK v2** as kernel in AML Notebook.\n",
    "\n",
    "##### Add the following environment variables to your .env file\n",
    "\n",
    "1. Document Intelligence variables (form recogniser)\n",
    "    - Getting the values from the `Resource Management`/ `Keys and Endpoint` blade in the *Document Intelligence* service in the Azure portal.\n",
    "    ```\n",
    "    FORM_RECOGNIZER_ENDPOINT= \n",
    "    FORM_RECOGNIZER_KEY= \n",
    "    ```\n",
    "\n",
    "1. Storage Account variables  \n",
    "    - Go to your storage resource in the porta l.\n",
    "        - For Connection string Security & Netowrking / Access Keys / Connection String\n",
    "    - Click on the newly created container   \n",
    "    - For SAS URL:  Settings, Shared access tokens  \n",
    "        - Adjust the expiry date (after the workshop finishes) and create a Shared access token for the container by clicking the *Generate SAS token and URL* button.  \n",
    "        - Copy SAS Url  \n",
    "    - \n",
    "    ```\n",
    "    BLOB_STORAGE_ACCOUNT_CONNECTION_STRING=\n",
    "    BLOB_SAS_URL=\n",
    "    \n",
    "    ``` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Install AI Doc Intelligence library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install azure-ai-formrecognizer python library and restart the kernel after installation\n",
    "%pip install azure-ai-formrecognizer --upgrade --user\n",
    "%pip install tabulate\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Setting up AI Document Intelligence endpoint and key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the os module for interacting with the operating system\n",
    "import os\n",
    "# Import for handling resource not found errors\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "# Import for authenticating with the Azure service\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "# Import formrecognizer library to analysis the docs\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient, AnalyzeResult\n",
    "\n",
    "# load the environments details\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Set `<your-endpoint>` and `<your-key>` variables with the values from the Azure portal\n",
    "# END_POINT is the endpoint URL of your AI Document Intelligence service\n",
    "# END_POINT_KEY is the key for your AI Document Intelligence service\n",
    "END_POINT = os.getenv(\"FORM_RECOGNIZER_ENDPOINT\")\n",
    "END_POINT_KEY = os.getenv(\"FORM_RECOGNIZER_KEY\")\n",
    "\n",
    "# Create a DocumentAnalysisClient instance\n",
    "# This client is used to interact with the Azure Form Recognizer service\n",
    "# It is initialized with your endpoint and key\n",
    "form_recognizer_client = DocumentAnalysisClient(END_POINT, AzureKeyCredential(END_POINT_KEY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Analysing the Layout Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL of the sample document to analyse.\n",
    "# You can change the URL to your sample layout docs but ensure you provide appropriate access\n",
    "#layoutUrl = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-REST-api-samples/master/curl/form-recognizer/sample-layout.pdf\"\n",
    "# Try out pdfs with interessting layouts and tables.\n",
    "\n",
    "blob_sas=os.getenv(\"BLOB_SAS_URL\")\n",
    "\n",
    "# Start the analysis of the document using the prebuilt layout model\n",
    "# The result is a poller object that can be used to check the status of the operation\n",
    "poller = form_recognizer_client.begin_analyze_document_from_url(\"prebuilt-layout\",blob_sas)\n",
    "print(poller)\n",
    "\n",
    "# Get the result of the analysis\n",
    "result = poller.result()\n",
    "\n",
    "# Extract document insights\n",
    "# Check if the document contains any handwritten content\n",
    "if any([style.is_handwritten for style in result.styles]):\n",
    "    print(\"Document contains handwritten content\")\n",
    "else:\n",
    "    print(\"Document does not contain handwritten content\")\n",
    "\n",
    "# Loop through each page in the document\n",
    "for page in result.pages:\n",
    "    print(f\"----Analyzing layout from page #{page.page_number}----\")\n",
    "    print(\n",
    "        f\"Page has width: {page.width} and height: {page.height}, measured with unit: {page.unit}\"\n",
    "    )\n",
    "    for line_idx, line in enumerate(page.lines):\n",
    "        words = line.get_words()\n",
    "        #words = get_words(page, line)\n",
    "        print(\n",
    "            f\"...Line # {line_idx} has word count {len(words)} and text '{line.content}' \"\n",
    "            f\"within bounding polygon '{line.polygon}'\"\n",
    "        )\n",
    "\n",
    "        # Loop through each word in the line  \n",
    "        for word in words:\n",
    "            print(\n",
    "                f\"......Word '{word.content}' has a confidence of {word.confidence}\"\n",
    "            )\n",
    "\n",
    "    # Loop through each selection mark in the page \n",
    "    for selection_mark in page.selection_marks:\n",
    "        print(\n",
    "            f\"Selection mark is '{selection_mark.state}' within bounding polygon \"\n",
    "            f\"'{selection_mark.polygon}' and has a confidence of {selection_mark.confidence}\"\n",
    "        )\n",
    "\n",
    "# Loop through each table in the document\n",
    "for table_idx, table in enumerate(result.tables):\n",
    "    print(\n",
    "        f\"Table # {table_idx} has {table.row_count} rows and \"\n",
    "        f\"{table.column_count} columns\"\n",
    "    )\n",
    "    # Loop through each bounding region of the table\n",
    "    for region in table.bounding_regions:\n",
    "        print(\n",
    "            f\"Table # {table_idx} location on page: {region.page_number} is {region.polygon}\"\n",
    "        )\n",
    "    # Loop through each cell in the table    \n",
    "    for cell in table.cells:\n",
    "        print(\n",
    "            f\"...Cell[{cell.row_index}][{cell.column_index}] has text '{cell.content}'\"\n",
    "        )\n",
    "\n",
    "        # Loop through each bounding region of the cell\n",
    "        for region in cell.bounding_regions:\n",
    "            print(\n",
    "                f\"...content on page {region.page_number} is within bounding polygon '{region.polygon}'\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Extracted Layout Document insights/ response as a JSON format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import JSON packages\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "from azure.core.serialization import AzureJSONEncoder\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "# generate the unique file name based on the current timestamp and the basename of the URL\n",
    "filename = datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d%H%M%S')+\"_\"+os.path.splitext(os.path.basename(urlparse(blob_sas).path))[0]\n",
    "\n",
    "# parse and format the model response json \n",
    "# convert the received model to a dictionary\n",
    "analyze_result_dict = result.to_dict()\n",
    "\n",
    "# save the dictionary as JSON content in a JSON file, use the AzureJSONEncoder\n",
    "# to help make types, such as dates, JSON serializable\n",
    "with open(str(filename), 'w') as f:\n",
    "        json.dump(analyze_result_dict, f, cls=AzureJSONEncoder,indent=4)\n",
    "\n",
    "# convert the dictionary back to the original model\n",
    "model = AnalyzeResult.from_dict(analyze_result_dict)\n",
    "print(\"--------------JSON Response from Model Starts---------------------\")\n",
    "# use the model as normal\n",
    "print(\"Model ID: '{}'\".format(model.model_id))\n",
    "print(\"Number of pages analyzed {}\".format(len(model.pages)))\n",
    "print(\"API version used: {}\".format(model.api_version))\n",
    "print(json.dumps(analyze_result_dict,cls=AzureJSONEncoder,indent=4))\n",
    "print(\"--------------JSON Response from Model Ends---------------------\")##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G. Extracted document insights/ response as table of Key Value Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the document insights as key / value table\n",
    "from tabulate import tabulate\n",
    "\n",
    "data = []\n",
    "\n",
    "# Display key value pairs\n",
    "for idx, document in enumerate(result.documents):\n",
    "    print()\n",
    "    print(\"--------Analyzing document #{}--------\".format(idx + 1))\n",
    "    print(\"Document has type {}\".format(document.doc_type))\n",
    "    print(\"Document has document type confidence {}\".format(\n",
    "        document.confidence))\n",
    "    print(\"Document was analyzed with model with ID {}\".format(\n",
    "        result.model_id))\n",
    "    print()\n",
    "    for name, field in document.fields.items():\n",
    "        field_value = field.value if field.value else field.content\n",
    "        if field.value_type != 'list':\n",
    "            data.append([name, field.value, field.confidence])\n",
    "\n",
    "data.sort()\n",
    "print(tabulate(data, headers=[\n",
    "    'Label', 'Value', 'Confidence'], tablefmt='fancy_grid'))\n",
    "\n",
    "# Display table data\n",
    "for i, table in enumerate(result.tables):\n",
    "\n",
    "    row_index = 1\n",
    "    hdr = []\n",
    "    rows = []\n",
    "    row = []\n",
    "\n",
    "    print(\"\\nTable {} can be found on page:\".format(i + 1))\n",
    "    # for region in table.bounding_regions:\n",
    "    #     print(\"...{}\".format(i + 1, region.page_number))\n",
    "\n",
    "    for cell in table.cells:\n",
    "        if cell.row_index == 0:\n",
    "            hdr.append(cell.content)\n",
    "        else:\n",
    "            if row_index != cell.row_index:\n",
    "                rows.append(row)\n",
    "                row_index = cell.row_index\n",
    "                row = []\n",
    "\n",
    "            row.append(cell.content)\n",
    "\n",
    "    rows.append(row)\n",
    "    print(tabulate(rows, headers=hdr, tablefmt='fancy_grid'))    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
